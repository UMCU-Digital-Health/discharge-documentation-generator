import json
import re

from openai import AzureOpenAI

from discharge_docs.prompts.prompt import (
    load_additional_information_prompt,
    load_missing_information_prompt,
    load_segment_prompt,
)


def compare_GPT_output_with_EPD_output(
    GPT_output: str,
    EPD_output: str,
    evaluation_prompt: str,
    engine: str,
    client: AzureOpenAI,
    temperature: float,
) -> dict:
    """Compare GPT output with EPD output through GPT request with generate evaluation
    prompt

    Parameters
    ----------
    GPT_output : str
        The output generated by the GPT model.
    EPD_output : str
        The output generated by the EPD system.
    evaluation_prompt : str
        The prompt for evaluation.
    engine : str
        The AI model engine to use.
    temperature : float
        The temperature parameter for AI model's response generation.

    Returns
    -------
    dict
        The generated evaluation prompt.
    """
    messages = [
        {"role": "user", "content": evaluation_prompt},
        {"role": "user", "content": "Samenvatting A: " + EPD_output},  # samenvatting A
        {"role": "user", "content": "Samenvatting B: " + GPT_output},  # samenvatting B
    ]
    response = client.chat.completions.create(
        model=engine,
        messages=messages,
        temperature=temperature,
    )
    reply = json.loads(
        re.sub(
            "```(json)?",
            "",
            response.model_dump()["choices"][0]["message"]["content"],
        )
    )
    return reply


def compare_two_texts(
    text_a: str,
    text_b: str,
    evaluation_prompt: str,
    engine: str,
    client: AzureOpenAI,
    temperature: float,
) -> dict:
    """Compare GPT output with EPD output through GPT request with generate evaluation
    prompt

    Parameters
    ----------
    GPT_output : str
        The output generated by the GPT model.
    EPD_output : str
        The output generated by the EPD system.
    evaluation_prompt : str
        The prompt for evaluation.
    engine : str
        The AI model engine to use.
    temperature : float
        The temperature parameter for AI model's response generation.

    Returns
    -------
    dict
        The generated evaluation prompt.
    """
    messages = [
        {"role": "user", "content": evaluation_prompt},
        {"role": "user", "content": "tekst A: " + text_a},  # samenvatting A
        {"role": "user", "content": "tekst B: " + text_b},  # samenvatting B
    ]
    response = client.chat.completions.create(
        model=engine,
        messages=messages,
        temperature=temperature,
    )

    return convert_response_to_list(response)


def compare_two_lists(
    list_a: str,
    list_b: str,
    evaluation_prompt: str,
    engine: str,
    client: AzureOpenAI,
    temperature: float,
) -> dict:
    """Compare GPT output with EPD output through GPT request with generate evaluation
    prompt

    Parameters
    ----------
    GPT_output : str
        The output generated by the GPT model.
    EPD_output : str
        The output generated by the EPD system.
    evaluation_prompt : str
        The prompt for evaluation.
    engine : str
        The AI model engine to use.
    temperature : float
        The temperature parameter for AI model's response generation.

    Returns
    -------
    dict
        The generated evaluation prompt.
    """
    list_a_string = convert_list_to_string(list_a)
    list_b_string = convert_list_to_string(list_b)

    messages = [
        {"role": "user", "content": evaluation_prompt},
        {"role": "user", "content": "lijst A: " + list_a_string},  # samenvatting A
        {"role": "user", "content": "lijst B: " + list_b_string},  # samenvatting B
    ]
    response = client.chat.completions.create(
        model=engine,
        messages=messages,
        temperature=temperature,
    )

    return convert_response_to_list(response)


def split_letter_into_segments(
    letter: str,
    engine: str,
    client: AzureOpenAI,
    temperature: float = 0.2,
) -> list:
    """Split the letter into segments based on topic

    Parameters
    ----------
    letter : str
        The letter to split into segments.
    engine : str
        The AI model engine to use.
    temperature : float, optional
        The temperature parameter for AI model's response generation,
            by default 0.2.

    Returns
    -------
    list
        The list of segments.
    """
    segment_prompt = load_segment_prompt()
    messages = [
        {"role": "user", "content": segment_prompt},
        {"role": "user", "content": "Brief: " + letter},
    ]
    response = client.chat.completions.create(
        model=engine,
        messages=messages,
        temperature=temperature,
    )

    return convert_response_to_list(response)


def convert_response_to_list(response, verbose=False):
    response_content = response.choices[0].message.content
    if verbose:
        print()
        print(response_content)
        print()
    response_list = json.loads(response_content)

    return response_list


def convert_list_to_string(list_of_strings):
    converted_list = "- " "\n - ".join(list_of_strings)

    return converted_list


def information_delta(
    text_1, text_2, deployment_name, client, delta_type, temperature=0.2
):
    if delta_type == "additional":
        prompt = load_additional_information_prompt()
    elif delta_type == "missing":
        prompt = load_missing_information_prompt()

    response = compare_two_texts(
        text_1,
        text_2,
        prompt,
        engine=deployment_name,
        client=client,
        temperature=temperature,
    )

    return response
