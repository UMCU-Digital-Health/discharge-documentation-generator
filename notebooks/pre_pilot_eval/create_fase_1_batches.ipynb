{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from discharge_docs.prompts.prompt import (\n",
    "    load_prompts,\n",
    "    load_template_prompt,\n",
    ")\n",
    "\n",
    "os.environ[\"TIKTOKEN_CACHE_DIR\"] = \"\"\n",
    "\n",
    "pd.options.display.max_colwidth = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current file path\n",
    "current_path = Path.cwd().parent\n",
    "relative_path = current_path / \"data\" / \"processed\" / \"metavision_data_april_dp.parquet\"\n",
    "df_discharge = pd.read_parquet(relative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompts:\n",
    "user_prompt, system_prompt = load_prompts()\n",
    "template_prompt_NICU = load_template_prompt(\"NICU\")\n",
    "template_prompt_IC = load_template_prompt(\"IC\")\n",
    "template_prompt_CAR = load_template_prompt(\"CAR\")\n",
    "template_prompt_PSY = load_template_prompt(\"PSY\")\n",
    "template_prompt_dict = {\n",
    "    \"nicu\": template_prompt_NICU,\n",
    "    \"ic\": template_prompt_IC,\n",
    "    \"car\": template_prompt_CAR,\n",
    "    \"psy\": template_prompt_PSY,\n",
    "    \"demo\": template_prompt_NICU,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discharge[\"department\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discharge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voeg token lenth toe:\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encs = []\n",
    "longer_encs = []\n",
    "for enc_id in np.sort(df_discharge[\"enc_id\"].unique()):\n",
    "    patient_data_string = \" \".join(\n",
    "        df_discharge[df_discharge[\"enc_id\"] == enc_id][\"value\"]\n",
    "    )\n",
    "    # print(\n",
    "    #     f\"The number of tokens in encounter {enc_id}:\n",
    "    #  {len(encoding.encode(patient_data_string))} \"+\n",
    "    #     f\"with length of stay: {df_discharge[df_discharge.enc_id == enc_id]\n",
    "    # .length_of_stay.unique()}\"\n",
    "    # )\n",
    "    template_prompt_length = len(\n",
    "        encoding.encode(user_prompt + system_prompt + template_prompt_NICU)\n",
    "    )\n",
    "    if (len(encoding.encode(patient_data_string)) + template_prompt_length) < 110000:\n",
    "        encs.append(enc_id)\n",
    "    else:\n",
    "        longer_encs.append(enc_id)\n",
    "\n",
    "# print(encs)\n",
    "print(longer_encs)  # remove these from batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the enc_ids that are too long for GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardcoded such that the assigment remains reproducible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_ids_hardcoded = [0, 1, 2, 3, 5, 6, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discharge[\"enc_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discharge = df_discharge[~df_discharge[\"enc_id\"].isin(longer_ids_hardcoded)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discharge[\"enc_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discharge_filtered = df_discharge[df_discharge[\"description\"] == \"Ontslagbrief\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discharge_filtered[df_discharge_filtered[\"enc_id\"] == 11][[\"enc_id\", \"value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IC = df_discharge_filtered[\n",
    "    df_discharge_filtered[\"department\"] == \"Intensive Care Centrum\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NICU = df_discharge_filtered[df_discharge_filtered[\"department\"] == \"Neonatologie\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NICU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NICU discharge letters contain some incomplete letters, so we will filter them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fist_4_ids_nicu = [201, 41, 244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_strings = [\"..\", \"G-schijf\", \"G-\", \"G schijf\"]\n",
    "\n",
    "potential_nicu_letters = []\n",
    "for _, row in df_NICU.iterrows():\n",
    "    if not any(substring in row[\"value\"] for substring in exclusion_strings):\n",
    "        potential_nicu_letters.append(row[\"enc_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(potential_nicu_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NICU[df_NICU[\"enc_id\"] == 38][[\"enc_id\", \"value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IC looks good. Can use random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fist_4_ids_ic = [373, 304, 437, 24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_IC[df_IC[\"enc_id\"] == 24][[\"enc_id\", \"value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_ic_letters = list(df_IC.enc_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eerste idee voor het sampelen van de ontslagbrieven:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We nemen een sample van 100 EPD/ontslagbrieven combinaties, 1 willekeurige, 33 van elke afdeling. Uniform verdeeld qua lengte tussen het kortste dossier en het langst acceptabele dossier voor GPT. 1 willekeurige wordt van tevoren door een arts geannoteerd, te gebruiken bij de kickoff. 6 (2 van elke afdeling) worden gebruikt voor de burn-in en dus gescoord door alle 3 de studenten en geÃ«valueerd tijdens de tweede sessie. De overige 93 (31 per afdeling) willen we verdelen over batches zodat mochten we niet tot de 100 komen in totaal we toch voldoende overlap tussen de studenten hebben en de studenten zowel de GPT als art brief die bij een opname hoort hebben gezien.\n",
    "\n",
    "Per batch van 30: 6 overlappende (2 elke afdeling) en 3x8 unieke per student. Deze in een random volgorde gezet over de GPT en arts brieven, dus per batch 2*(6+8) = 28 te annoteren brieven per student in willekeurige volgorde. de laatste brief kan er bij een willekeurige student bij komen. Deze batches kunnen allemaal van tevoren worden gegenereerd zodat de studenten niet hoeven te wachten na een batch, maar zodat er wel voldoende overlap blijft bestaan en zowel de GPT en arts brief door dezelfde student zijn beoordeeld binnen een batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aangepaste versie**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doordat er wat dingen anders zijn gelopen dan in het initiele plan, is de opzet van het batchen ook wat veranderd. Het idee is dat de gewenste overlap gelijk blijft: 20%. We hebben voor de kickoff patient een nep patient gebruikt, dus deze hoefde niet gexcludeerd te worden. We hebben in eerste instantie 3 IDS van de NICU voorgelegd en 4 van de IC. Deze hebben we voor het maken van de volgende batches eruit gefilterd. Voor het samplen gebruiken we ook niet de token length van het dossier, maar de LoS als proxy. Dit zorgt ervoor dat we niet met bins hoeven werken.\n",
    "\n",
    "In de nieuwe methode worden eerst dossiers uniform over de LoS gesampled en deze daarna over de studenten verdeeld, dusdanig dat ~elke vijfde brief van elke afdeling overlapt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_nicu_letters = set(potential_nicu_letters) - set(fist_4_ids_nicu)\n",
    "remaining_ic_letters = set(potential_ic_letters) - set(fist_4_ids_ic)\n",
    "print(f\"Length remaining nicu letters: {len(remaining_nicu_letters)}\")\n",
    "print(f\"Length remaining ic letters: {len(remaining_ic_letters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NICU_processed = df_NICU[df_NICU[\"enc_id\"].isin(remaining_nicu_letters)][\n",
    "    [\"enc_id\", \"length_of_stay\"]\n",
    "]\n",
    "df_IC_processed = df_IC[df_IC[\"enc_id\"].isin(remaining_ic_letters)][\n",
    "    [\"enc_id\", \"length_of_stay\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ordered_id_list(df_los):\n",
    "    \"\"\"Return the list of patient IDs for that has been sampled uniformly based on LoS\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_los : pd.DataFrame\n",
    "        dataframe containing enc_id and\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of NICU letters in which order they are to be divided among the\n",
    "    # students\n",
    "    # First, from the dataframe with enc_id and length of stay, create a dict for which\n",
    "    # the\n",
    "    # keys are the length and the value a list of enc_ids with that length\n",
    "    # shuffle the list of enc_ids for each length\n",
    "\n",
    "    length_enc_id_dict = {}\n",
    "    for _, row in df_los.iterrows():\n",
    "        length = row[\"length_of_stay\"]\n",
    "        if length not in length_enc_id_dict:\n",
    "            length_enc_id_dict[length] = []\n",
    "        length_enc_id_dict[length].append(row[\"enc_id\"])\n",
    "\n",
    "    for key in length_enc_id_dict:\n",
    "        random.shuffle(length_enc_id_dict[key])\n",
    "\n",
    "    # Now randomly sample one of the keys in the length_enc_id_dict and pop the first\n",
    "    # element. If the list is empty, remove the key from the dict. Add the popped\n",
    "    # element\n",
    "    # to the ordered_id list. Repeat until the dict is empty\n",
    "\n",
    "    ordered_ids = []\n",
    "    while length_enc_id_dict:\n",
    "        length = random.choice(list(length_enc_id_dict.keys()))\n",
    "        ordered_ids.append(length_enc_id_dict[length].pop(0))\n",
    "        if not length_enc_id_dict[length]:\n",
    "            length_enc_id_dict.pop(length)\n",
    "\n",
    "    return ordered_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a list with students, and a dict with departments and ids\n",
    "# divide the ids among the students such that every overlap_rounds all students get the\n",
    "#  same\n",
    "# id for the department\n",
    "\n",
    "\n",
    "def divide_ids_among_students(students, department_id_dict, overlap_rounds=5):\n",
    "    \"\"\"Take a list with students, and a dict with departments and ids\n",
    "    divide the ids among the students such that every overlap_rounds all\n",
    "    students get the same id for the department.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    students : list\n",
    "        list of students\n",
    "    department_id_dict : dict\n",
    "        dict with department as key and list of ids as value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        dict with student as key and list of ids as value\n",
    "    \"\"\"\n",
    "\n",
    "    student_id_dict = {}\n",
    "    student_dept_dict = {}\n",
    "    for student in students:\n",
    "        student_id_dict[student] = []\n",
    "        student_dept_dict[student] = []\n",
    "\n",
    "    # For determining when an overlapping id needs to be assigned\n",
    "    department_overlap_dict = {}\n",
    "    for dep in department_id_dict:\n",
    "        department_overlap_dict[dep] = overlap_rounds\n",
    "\n",
    "    empty_dep = False\n",
    "    while not empty_dep:\n",
    "\n",
    "        for id in department_id_dict:\n",
    "            if department_overlap_dict[id] // overlap_rounds > 0:\n",
    "\n",
    "                id_to_add = department_id_dict[id].pop(0)\n",
    "                department_overlap_dict[id] -= overlap_rounds\n",
    "\n",
    "                # stop when one of the departments has no more ids to assign\n",
    "                if not department_id_dict[id]:\n",
    "                    empty_dep = True\n",
    "\n",
    "                for student in students:\n",
    "                    student_id_dict[student].append(id_to_add)\n",
    "                    student_dept_dict[student].append(id)\n",
    "            else:\n",
    "                for student in students:\n",
    "\n",
    "                    id_to_add = department_id_dict[id].pop(0)\n",
    "                    department_overlap_dict[id] += 1\n",
    "\n",
    "                    # stop when one of the departments has no more ids to assign\n",
    "                    if not department_id_dict[id]:\n",
    "                        empty_dep = True\n",
    "                        break\n",
    "\n",
    "                    student_id_dict[student].append(id_to_add)\n",
    "                    student_dept_dict[student].append(id)\n",
    "\n",
    "    return student_id_dict, student_dept_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure consistency\n",
    "random.seed(1000)\n",
    "\n",
    "nicu_ordered = create_ordered_id_list(df_NICU_processed)\n",
    "ic_ordered = create_ordered_id_list(df_IC_processed)\n",
    "department_dict = {\"NICU\": nicu_ordered, \"IC\": ic_ordered}\n",
    "students = [\"student_1\", \"student_2\"]\n",
    "\n",
    "id_assignment, dpt_assignment = divide_ids_among_students(students, department_dict, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_student_1 = id_assignment[\"student_1\"]\n",
    "dpt_student_1 = dpt_assignment[\"student_1\"]\n",
    "print(ids_student_1)\n",
    "print(dpt_student_1)\n",
    "ids_student_2 = id_assignment[\"student_2\"]\n",
    "dpt_student_2 = dpt_assignment[\"student_2\"]\n",
    "print(ids_student_2)\n",
    "print(dpt_student_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
